---
title: Quantum Machine Learning-Bibliothek
description: Erfahren Sie, wie Machine Learning auf Quantum-Systemen verwendet wird.
author: alexeib2
ms.author: alexeib
ms.date: 11/22/2019
ms.topic: conceptual
uid: microsoft.quantum.libraries.machine-learning.intro
no-loc:
- Q#
- $$v
ms.openlocfilehash: e2f4a4a63eef40474856426b3b29652b5d3053b2
ms.sourcegitcommit: 71605ea9cc630e84e7ef29027e1f0ea06299747e
ms.translationtype: MT
ms.contentlocale: de-DE
ms.lasthandoff: 01/26/2021
ms.locfileid: "98854031"
---
# <a name="introduction-to-quantum-machine-learning"></a><span data-ttu-id="39d51-103">Einführung in Quantum Machine Learning</span><span class="sxs-lookup"><span data-stu-id="39d51-103">Introduction to Quantum Machine Learning</span></span>

## <a name="framework-and-goals"></a><span data-ttu-id="39d51-104">Framework und Ziele</span><span class="sxs-lookup"><span data-stu-id="39d51-104">Framework and goals</span></span>

<span data-ttu-id="39d51-105">Die Quantum-Codierung und-Verarbeitung von Informationen ist eine leistungsfähige Alternative zu klassischen Machine Learning-Quantum-Klassifizierungen.</span><span class="sxs-lookup"><span data-stu-id="39d51-105">Quantum encoding and processing of information is a powerful alternative to classical machine learning Quantum classifiers.</span></span> <span data-ttu-id="39d51-106">Dies ermöglicht es uns, Daten in Quantum-Registern zu codieren, die relativ zur Anzahl der Features präzise sind. dabei wird die Quantum-jede Verflechtungen systematisch als Berechnungs Ressource verwendet und die Quantum-Messung für den Klassen Rückschluss verwendet.</span><span class="sxs-lookup"><span data-stu-id="39d51-106">In particular, it allows us to encode data in quantum registers that are concise relative to the number of features, systematically employing quantum entanglement as computational resource and employing quantum measurement for class inference.</span></span>
<span data-ttu-id="39d51-107">Die Verbindungs zentrierte Quantum-Klassifizierung ist eine relativ einfache Quantum-Lösung, die die Daten Codierung mit einer schnellen Entangling/Disentangling-Quantum-Verbindung gefolgt von Messungen zum Ableiten von Klassen Bezeichnungen von Daten Beispielen kombiniert.</span><span class="sxs-lookup"><span data-stu-id="39d51-107">Circuit centric quantum classifier is a relatively simple quantum solution that combines data encoding with a rapidly entangling/disentangling quantum circuit followed by measurement to infer class labels of data samples.</span></span>
<span data-ttu-id="39d51-108">Das Ziel besteht darin, die klassische Charakterisierung und Speicherung von Themen Verbindungen zu gewährleisten sowie hybride Quantum/klassische Schulungen der Verbindungsparameter auch für extrem große Merkmals Räume zu gewährleisten.</span><span class="sxs-lookup"><span data-stu-id="39d51-108">The goal is to ensure classical characterization and storage of subject circuits, as well as hybrid quantum/classical training of the circuit parameters even for extremely large feature spaces.</span></span>

## <a name="classifier-architecture"></a><span data-ttu-id="39d51-109">Klassifizierungs Architektur</span><span class="sxs-lookup"><span data-stu-id="39d51-109">Classifier architecture</span></span>

<span data-ttu-id="39d51-110">Bei der Klassifizierung handelt es sich um eine überwachte Machine Learning-Aufgabe, bei der das Ziel darin besteht, Klassen Bezeichnungen $ \{ Y_1, y_2, \ldots y_d \} $ bestimmter Daten Beispiele abzuleiten.</span><span class="sxs-lookup"><span data-stu-id="39d51-110">Classification is a supervised machine learning task, where the goal is to infer class labels $\{y_1,y_2,\ldots,y_d\}$ of certain data samples.</span></span> <span data-ttu-id="39d51-111">Das Trainings Dataset ist eine Sammlung von Beispielen $ \mathcal{d} = \{ (x, y)} $ mit bekannten vorzugewiesenen Bezeichnungen.</span><span class="sxs-lookup"><span data-stu-id="39d51-111">The "training data set" is a collection of samples $\mathcal{D}=\{(x,y)}$ with known pre-assigned labels.</span></span> <span data-ttu-id="39d51-112">Hier $x $ ein Daten Sample, und $y $ ist die bekannte Bezeichnung "Trainings Bezeichnung".</span><span class="sxs-lookup"><span data-stu-id="39d51-112">Here $x$ is a data sample and $y$ is its known label called "training label".</span></span>
<span data-ttu-id="39d51-113">Ähnlich wie herkömmliche Methoden besteht die Quantum-Klassifizierung aus drei Schritten:</span><span class="sxs-lookup"><span data-stu-id="39d51-113">Somewhat similar to traditional methods, quantum classification consists of three steps:</span></span>
- <span data-ttu-id="39d51-114">Daten Codierung</span><span class="sxs-lookup"><span data-stu-id="39d51-114">data encoding</span></span>
- <span data-ttu-id="39d51-115">Vorbereitung eines Klassifizierungs Zustands</span><span class="sxs-lookup"><span data-stu-id="39d51-115">preparation of a classifier state</span></span>
- <span data-ttu-id="39d51-116">Messung aufgrund der Wahrscheinlichkeits Natur der Messung müssen diese drei Schritte mehrmals wiederholt werden.</span><span class="sxs-lookup"><span data-stu-id="39d51-116">measurement Due to the probabilistic nature of the measurement, these three steps must be repeated multiple times.</span></span> <span data-ttu-id="39d51-117">Sowohl die Codierung als auch das Berechnen des Klassifizierungs Zustands werden mithilfe von *Quantum*-Verbindungen durchgeführt.</span><span class="sxs-lookup"><span data-stu-id="39d51-117">Both the encoding and the computing of the classifier state are done by means of *quantum circuits*.</span></span> <span data-ttu-id="39d51-118">Während die Codierungs Verbindung in der Regel Daten gesteuert und Parameter frei ist, enthält die Klassifizierungs Leitung einen ausreichenden Satz von erlernbaren Parametern.</span><span class="sxs-lookup"><span data-stu-id="39d51-118">While the encoding circuit is usually data-driven and parameter-free, the classifier circuit contains a sufficient set of learnable parameters.</span></span> 

<span data-ttu-id="39d51-119">In der vorgeschlagenen Lösung besteht die Klassifizierungs Leitung aus Single-Qubit-Drehungen und zwei-Qubit-gesteuerten Rotationen.</span><span class="sxs-lookup"><span data-stu-id="39d51-119">In the proposed solution the classifier circuit is composed of single-qubit rotations and two-qubit controlled rotations.</span></span> <span data-ttu-id="39d51-120">Die erlernbaren Parameter sind die Drehwinkel.</span><span class="sxs-lookup"><span data-stu-id="39d51-120">The learnable parameters here are the rotation angles.</span></span> <span data-ttu-id="39d51-121">Die Drehung und kontrollierten Drehungs Gates sind bekanntermaßen für die Quantum-Berechnung *universell* , was bedeutet, dass jede einheitliche Gewichtungs Matrix in eine lange genug Verbindung zerlegt werden kann, die aus solchen Gates besteht.</span><span class="sxs-lookup"><span data-stu-id="39d51-121">The rotation and controlled rotation gates are known to be *universal* for quantum computation, which means that any unitary weight matrix can be decomposed into a long enough circuit consisting of such gates.</span></span>

<span data-ttu-id="39d51-122">In der vorgeschlagenen Version wird nur eine Verbindung gefolgt von einer einzelnen Frequenz Schätzung unterstützt.</span><span class="sxs-lookup"><span data-stu-id="39d51-122">In the proposed version, only one circuit followed by a single frequency estimation is supported.</span></span>
<span data-ttu-id="39d51-123">Folglich ist die Lösung eine Quantum-Analog eines Unterstützungs Vektor Computers mit einem Low-Degree-Polynoms-Kernel.</span><span class="sxs-lookup"><span data-stu-id="39d51-123">Thus, the solution is a quantum analog of a support vector machine with a low-degree polynomial kernel.</span></span>

![Mehrschichtiges Perzeptron und Circuit zentrierter Klassifizierer](~/media/DLvsQCC.png)

<span data-ttu-id="39d51-125">Ein einfacher Quantum Klassifizierungs-Entwurf kann mit einer herkömmlichen SVM-Lösung (Support Vector Machine) verglichen werden.</span><span class="sxs-lookup"><span data-stu-id="39d51-125">A simple quantum classifier design can be compared to a traditional support vector machine (SVM) solution.</span></span> <span data-ttu-id="39d51-126">Der Rückschluss für ein Daten Beispiel $x $ im Fall von SVM erfolgt mithilfe der optimalen Kernel Form $ \sum \ alpha_j k (x_j, x) $, wobei $k $ eine bestimmte Kernel Funktion ist.</span><span class="sxs-lookup"><span data-stu-id="39d51-126">The inference for a data sample $x$ in case of SVM is done using an optimal kernel form $\sum \alpha_j  k(x_j,x)$ where $k$ is a certain kernel function.</span></span>

<span data-ttu-id="39d51-127">Im Gegensatz dazu verwendet ein Quantum-Klassifizierer den Prätor $p (y │ x, U (\urta)) = 〈 U (\urta) x | M | U (\urta) x 〉 $, der in Spirit ähnlich ist, aber technisch genau anders ist.</span><span class="sxs-lookup"><span data-stu-id="39d51-127">By contrast, a quantum classifier uses the predictor $p(y│x,U(\theta))=〈U(\theta)x|M|U(\theta)x〉$, which is similar in spirit but technically quite different.</span></span> <span data-ttu-id="39d51-128">Wenn also eine einfache Amplitude-Codierung verwendet wird, ist $p (y │ x, U (\theta)) $ ein quadratisches Formular in den verstärken $x $, aber die Koeffizienten dieses Formulars werden nicht mehr unabhängig erlernt. Sie werden stattdessen aus den Matrixelementen der Verbindungs $U (\orta) $ aggregiert, die in der Regel deutlich weniger learnable-Parameter $ \orta $ als die Dimension des Vektors $x $.</span><span class="sxs-lookup"><span data-stu-id="39d51-128">Thus, when a straightforward amplitude encoding is used,  $p(y│x,U(\theta))$ is a quadratic form in the amplitudes of $x$, but the coefficients of this form are no longer learned independently; they are instead aggregated from the matrix elements of the circuit $U(\theta)$, which typically has significantly fewer learnable parameters $\theta$ than the dimension of the vector $x$.</span></span> <span data-ttu-id="39d51-129">Der polynomale Grad des $p (y │ x, U (\urta)) $ in den ursprünglichen Funktionen kann auf $2 ^ l $ erweitert werden, indem eine Quantum-Produktcodierung in $l $-Kopien von $x $ verwendet wird.</span><span class="sxs-lookup"><span data-stu-id="39d51-129">The polynomial degree of $p(y│x,U(\theta))$ in the original features can be increased to $2^l$ by using a quantum product encoding on $l$ copies of $x$.</span></span>

<span data-ttu-id="39d51-130">Unsere Architektur untersucht relativ flache Verbindungen, die daher *schnell entfangen* müssen, um alle Korrelationen zwischen den Daten Features in allen Bereichen aufzuzeichnen.</span><span class="sxs-lookup"><span data-stu-id="39d51-130">Our architecture explores relatively shallow circuits, which therefore must be *rapidly entangling* in order to capture all the correlations between the data features at all ranges.</span></span> <span data-ttu-id="39d51-131">In der folgenden Abbildung ist ein Beispiel für die sehr nützliche schnell Verlaufs-Leitungs Komponente dargestellt.</span><span class="sxs-lookup"><span data-stu-id="39d51-131">An example of the most useful rapidly entangling circuit component is shown on figure below.</span></span> <span data-ttu-id="39d51-132">Obwohl eine Verbindung mit dieser Geometrie nur aus $3 n + 1 $ Gates besteht, gewährleistet die einheitliche Gewichtungs Matrix, die Sie berechnet, eine bedeutende Kreuz Diskussion zwischen den $2 ^ n $-Features.</span><span class="sxs-lookup"><span data-stu-id="39d51-132">Even though a circuit with this geometry consists of only $3 n+1$ gates, the unitary weight matrix that it computes ensures significant cross-talk between $2^n$ features.</span></span>

![Das schnelle entwirren von Quantum Circuit auf 5 Qubits (mit zwei zyklischen Ebenen).](~/media/5-qubit-qccc.png)

<span data-ttu-id="39d51-134">Die Verbindung im obigen Beispiel besteht aus 6 Single-Qubit Gates $ (G_1, \ldots, G_5; G_ {16} ) $ und 10 2-Qubits Gates $ (G_6, \ldots, g_ {15} ) $.</span><span class="sxs-lookup"><span data-stu-id="39d51-134">The circuit in the above example consists of 6 single-qubit gates $(G_1,\ldots,G_5; G_{16})$ and 10 two-qubits gates $(G_6,\ldots,G_{15})$.</span></span> <span data-ttu-id="39d51-135">Angenommen, jedes der Gates ist mit einem learnable-Parameter definiert. Wir haben 16 learnable-Parameter, während die Dimension des 5-Qubit-Hilbert-Raums 32 ist.</span><span class="sxs-lookup"><span data-stu-id="39d51-135">Assuming that each of the gates is defined with one learnable parameter we have 16 learnable parameters, while the dimension of the 5-qubit Hilbert space is 32.</span></span> <span data-ttu-id="39d51-136">Eine solche Schaltflächen Geometrie kann problemlos in beliebige $n $-Qubit-Register verallgemeinert werden, wenn $n $ ungerade ist, und gibt Verbindungen mit $3 n + 1 $-Parametern für $2 ^ n $-dimensionaler Merkmals Raum.</span><span class="sxs-lookup"><span data-stu-id="39d51-136">Such circuit geometry can be easily generalized to any $n$-qubit register, when $n$ is odd, yielding circuits with $3 n+1$ parameters for $2^n$-dimensional feature space.</span></span>

## <a name="classifier-training-as-a-supervised-learning-task"></a><span data-ttu-id="39d51-137">Klassifizierungs Training als beaufsichtigte lernaufgabe</span><span class="sxs-lookup"><span data-stu-id="39d51-137">Classifier training as a supervised learning task</span></span>

<span data-ttu-id="39d51-138">Das Trainieren eines Klassifizierungs Modells umfasst das Auffinden optimaler Werte der operativen Parameter, sodass Sie die durchschnittliche Wahrscheinlichkeit maximieren, dass die richtigen Trainings Bezeichnungen in den Trainings Beispielen abgeleitet werden.</span><span class="sxs-lookup"><span data-stu-id="39d51-138">Training of a classifier model involves finding optimal values of its operational parameters, such that they maximize the average likelihood of inferring the correct training labels across the training samples.</span></span>
<span data-ttu-id="39d51-139">Hier betreffen wir nur die Klassifizierung mit zwei Ebenen, d. h. die Groß-/Kleinschreibung von $d = $2 und nur zwei Klassen mit den Bezeichnungen $y _1, y_2 $.</span><span class="sxs-lookup"><span data-stu-id="39d51-139">Here, we concern ourselves with two level classification only, i.e. the case of $d=2$ and only two classes with the labels $y_1,y_2$.</span></span>

> [!NOTE]
> <span data-ttu-id="39d51-140">Eine prinzipielle Methode der Generalisierung unserer Methoden für eine beliebige Anzahl von Klassen besteht darin, Qubits durch qudits, d. h. durch $d $-Basiszustände, und die bidirektionale Messung mit $d $-Way-Messung zu ersetzen.</span><span class="sxs-lookup"><span data-stu-id="39d51-140">A principled way of generalizing our methods to arbitrary number of classes is to replace qubits with qudits, i.e. quantum units with $d$ basis states, and the two-way measurement with $d$-way measurement.</span></span>

### <a name="likelihood-as-the-training-goal"></a><span data-ttu-id="39d51-141">Wahrscheinlichkeit als Trainingsziel</span><span class="sxs-lookup"><span data-stu-id="39d51-141">Likelihood as the training goal</span></span>

<span data-ttu-id="39d51-142">Bei einem Informations fähigen Quantum Circuit $U (\orta) $, wobei $ \thta $ ein Vektor von Parametern ist, und die abschließende Messung durch $M $ bezeichnet, ist die durchschnittliche Wahrscheinlichkeit, dass der richtige Bezeichnungs Schluss ergibt, $ $ \begin{align} \mathcal{l} (\teta) = \frac {1} {| \mathcal{d} |} \left (\ sum_ {(x, Y_1) \in\mathcal{D}} P (M = Y_1 | U (\orta) x) + \ sum_ {(x, y_2) \in\mathcal{D}} P (M = y_2 | U (\urta) x) \right) \end{align} $ $ WHERE $P (M = y | z) $ ist die Wahrscheinlichkeit, $y $ im Quantum-Status $z $ zu messen.</span><span class="sxs-lookup"><span data-stu-id="39d51-142">Given a learnable quantum circuit $U(\theta)$, where $\theta$ is a vector of parameters, and denoting the final measurement by $M$, the average likelihood of the correct label inference is $$ \begin{align} \mathcal{L}(\theta)=\frac{1}{|\mathcal{D}|} \left( \sum_{(x,y_1)\in\mathcal{D}} P(M=y_1|U(\theta) x) + \sum_{(x,y_2)\in\mathcal{D}} P(M=y_2|U(\theta) x)\right) \end{align} $$ where $P(M=y|z)$ is the probability of measuring $y$ in quantum state $z$.</span></span>
<span data-ttu-id="39d51-143">Hier genügt es zu verstehen, dass die Wahrscheinlichkeitsfunktion $ \mathcal{l} (\teta) $ in $ \teta $ reibungslos ist und die Ableitung in $ \ theta_j $ im Grunde dasselbe Quantum-Protokoll berechnet werden kann, das auch zum Berechnen der Wahrscheinlichkeitsfunktion verwendet wird.</span><span class="sxs-lookup"><span data-stu-id="39d51-143">Here, it suffices to understand that the likelihood function $\mathcal{L}(\theta)$ is smooth in $\theta$ and its derivative in any $\theta_j$ can be computed by essentially the same quantum protocol as used for computing the likelihood function itself.</span></span> <span data-ttu-id="39d51-144">Dies ermöglicht die Optimierung von $ \mathcal{l} (\teta) $ by Gradient Descent.</span><span class="sxs-lookup"><span data-stu-id="39d51-144">This allows for optimizing the $\mathcal{L}(\theta)$ by gradient descent.</span></span>

### <a name="classifier-bias-and-training-score"></a><span data-ttu-id="39d51-145">Klassifizierungs-und Trainings Bewertung</span><span class="sxs-lookup"><span data-stu-id="39d51-145">Classifier bias and training score</span></span>

<span data-ttu-id="39d51-146">Bei einigen zwischen Werten (oder Endwerten) der Parameter in $ \teta $ muss ein einzelner realer Wert identifiziert werden, $b $ als *klassifizierungsbias* für die Rückschlüsse bekannt ist.</span><span class="sxs-lookup"><span data-stu-id="39d51-146">Given some intermediate (or final) values of the parameters in $\theta$, we need to identify a single real value $b$ know as *classifier bias* to do the inference.</span></span> <span data-ttu-id="39d51-147">Die Regel für die Bezeichnung der Bezeichnung funktioniert wie folgt:</span><span class="sxs-lookup"><span data-stu-id="39d51-147">The label inference rule works as follows:</span></span> 
- <span data-ttu-id="39d51-148">Ein Beispiel $x $ wird die Bezeichnung $y _2 $ if und nur dann zugewiesen, wenn $P (M = y_2 | U (\erta) x) + b > $0,5 (Regel 1) (andernfalls wird die Bezeichnung $y _1 $ zugewiesen)</span><span class="sxs-lookup"><span data-stu-id="39d51-148">A sample $x$ is assigned label $y_2$ if and only if $P(M=y_2|U(\theta) x) + b > 0.5$  (RULE1) (otherwise it is assigned label $y_1$)</span></span>

<span data-ttu-id="39d51-149">Eindeutig $b $ muss das Intervall von $ (-0,5, + 0,5) $ aufweisen, damit es sinnvoll ist.</span><span class="sxs-lookup"><span data-stu-id="39d51-149">Clearly $b$ must be in the interval $(-0.5,+0.5)$ to be meaningful.</span></span>

<span data-ttu-id="39d51-150">Ein Trainings Fall $ (x, y) \in \mathcal{d} $ wird bei Berücksichtigung der Bias-$b $ als fehl *Klassifizierung* betrachtet, wenn sich die Bezeichnung, die für $x $ as per Regel 1 abgeleitet wurde, tatsächlich von $y $ unterscheidet.</span><span class="sxs-lookup"><span data-stu-id="39d51-150">A training case $(x,y) \in \mathcal{D}$ is considered a *misclassification* given the bias $b$ if the label inferred for $x$ as per RULE1 is actually different from $y$.</span></span> <span data-ttu-id="39d51-151">Die Gesamtanzahl der fehl Klassifizierungen ist die *Trainings Bewertung* des Klassifizierers, bei der der Bias $b $ liegt.</span><span class="sxs-lookup"><span data-stu-id="39d51-151">The overall number of misclassifications is the *training score* of the classifier given the bias $b$.</span></span> <span data-ttu-id="39d51-152">Der *optimale* Klassifizierungs Aufwand $b $ minimiert die Trainings Bewertung.</span><span class="sxs-lookup"><span data-stu-id="39d51-152">The *optimal* classifier bias $b$ minimizes the training score.</span></span> <span data-ttu-id="39d51-153">Wenn die Voraus berechnete Wahrscheinlichkeitsschätzung $ \{ P (M = y_2 | U (\orta) x) | (x, \*) \in\mathcal{d} \} $. die optimale klassifizierungsabweichung kann durch die binäre Suche im Intervall von $ (-0,5, + 0,5) $ gefunden werden, indem höchstens $ \ Log_2 (| \mathcal{d} |) festgestellt wird. $ Steps.</span><span class="sxs-lookup"><span data-stu-id="39d51-153">It is easy to see that, given the precomputed probability estimates $\{ P(M=y_2|U(\theta) x) | (x,\*)\in\mathcal{D} \}$, the optimal classifier bias can be found by binary search in interval $(-0.5,+0.5)$ by making at most $\log_2(|\mathcal{D}|)$ steps.</span></span>

### <a name="reference"></a><span data-ttu-id="39d51-154">Verweis</span><span class="sxs-lookup"><span data-stu-id="39d51-154">Reference</span></span>

<span data-ttu-id="39d51-155">Diese Informationen sollten ausreichen, um mit dem Code zu beginnen.</span><span class="sxs-lookup"><span data-stu-id="39d51-155">This information should be enough to start playing with the code.</span></span> <span data-ttu-id="39d51-156">Wenn Sie jedoch mehr über dieses Modell erfahren möchten, lesen Sie den ursprünglichen Vorschlag: [ *"Circuit-zentrierte Quantum Classifiers", Maria Schuld, Alex Bocharov, Krysta svore und Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span><span class="sxs-lookup"><span data-stu-id="39d51-156">However, if you want to learn more about this model, please read the original proposal: [*'Circuit-centric quantum classifiers', Maria Schuld, Alex Bocharov, Krysta Svore and Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span></span>

<span data-ttu-id="39d51-157">Zusätzlich zum Codebeispiel, das Sie in den nächsten Schritten sehen werden, können Sie in [diesem Tutorial](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification) auch mit der Erkundung der Quantum-Klassifizierung beginnen.</span><span class="sxs-lookup"><span data-stu-id="39d51-157">In addition to the code sample you will see in the next steps, you can also start exploring quantum classification in [this tutorial](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification)</span></span> 
