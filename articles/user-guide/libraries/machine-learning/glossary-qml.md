---
title: Quantum Machine Learning-Bibliothek Glossar
description: Glossar der Quantum Machine Learning-Begriffe
author: alexeib2
ms.author: alexeib
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
no-loc:
- Q#
- $$v
ms.openlocfilehash: 39974af0121a5167f1965e508cd595535178548b
ms.sourcegitcommit: 9b0d1ffc8752334bd6145457a826505cc31fa27a
ms.translationtype: MT
ms.contentlocale: de-DE
ms.lasthandoff: 09/21/2020
ms.locfileid: "90833902"
---
# <a name="quantum-machine-learning-glossary"></a><span data-ttu-id="df4ad-103">Quantum Machine Learning Glossar</span><span class="sxs-lookup"><span data-stu-id="df4ad-103">Quantum Machine Learning glossary</span></span>

<span data-ttu-id="df4ad-104">Das Training eines Circuit-zentrierten quantumers ist ein Prozess mit vielen beweglichen Teilen, bei denen die gleiche (oder etwas größere) Menge an Kalibrierung nach Testversion und Fehler als das Training herkömmlicher Klassifizierungen erforderlich ist.</span><span class="sxs-lookup"><span data-stu-id="df4ad-104">Training of a circuit-centric quantum classifier is a process with many moving parts that require the same (or slightly larger) amount of calibration by trial and error as training of traditional classifiers.</span></span> <span data-ttu-id="df4ad-105">Hier werden die Hauptkonzepte und-Komponenten dieses Schulungsprozesses definiert.</span><span class="sxs-lookup"><span data-stu-id="df4ad-105">Here we define the main concepts and ingredients of this training process.</span></span>

## <a name="trainingtesting-schedules"></a><span data-ttu-id="df4ad-106">Trainings-/Testzeitpläne</span><span class="sxs-lookup"><span data-stu-id="df4ad-106">Training/testing schedules</span></span>

<span data-ttu-id="df4ad-107">Im Kontext des Klassifizierungs Trainings wird in einem *Zeitplan* eine Teilmenge der Daten Stichproben in einem allgemeinen Trainings-oder Testsatz beschrieben.</span><span class="sxs-lookup"><span data-stu-id="df4ad-107">In the context of classifier training a *schedule* describes a subset of data samples in an overall training or testing set.</span></span> <span data-ttu-id="df4ad-108">Ein Zeitplan wird normalerweise als Sammlung von Beispiel Indizes definiert.</span><span class="sxs-lookup"><span data-stu-id="df4ad-108">A schedule is usually defined as a collection of sample indices.</span></span>

## <a name="parameterbias-scores"></a><span data-ttu-id="df4ad-109">Parameter/Bias-Ergebnisse</span><span class="sxs-lookup"><span data-stu-id="df4ad-109">Parameter/bias scores</span></span>

<span data-ttu-id="df4ad-110">Wenn Sie einen Kandidaten Parameter Vektor und einen Klassifizierungs-Bias haben, wird das *Validierungs Ergebnis* in Relation zu einem ausgewählten Überprüfungs Zeitplan s gemessen und durch eine Reihe von fehl Klassifizierungen ausgedrückt, die über alle Stichproben im Zeitplan hinaus gezählt werden.</span><span class="sxs-lookup"><span data-stu-id="df4ad-110">Given a candidate parameter vector and a classifier bias, their *validation score* is measured relative to a chosen validation schedule S and is expressed by a number of misclassifications counted over all the samples in the schedule S.</span></span>

## <a name="hyperparameters"></a><span data-ttu-id="df4ad-111">Hyperparameter</span><span class="sxs-lookup"><span data-stu-id="df4ad-111">Hyperparameters</span></span>

<span data-ttu-id="df4ad-112">Der Modell Trainingsprozess unterliegt bestimmten voreingestellten Werten, die als *Hyperparameter*bezeichnet werden:</span><span class="sxs-lookup"><span data-stu-id="df4ad-112">The model training process is governed by certain pre-set values called *hyperparameters*:</span></span>

### <a name="learning-rate"></a><span data-ttu-id="df4ad-113">Learning rate (Lernrate)</span><span class="sxs-lookup"><span data-stu-id="df4ad-113">Learning rate</span></span>

<span data-ttu-id="df4ad-114">Dabei handelt es sich um einen der wichtigsten Hyperparameter.</span><span class="sxs-lookup"><span data-stu-id="df4ad-114">It is one of the key hyperparameters.</span></span> <span data-ttu-id="df4ad-115">Es definiert, wie viel aktuelle stochastischen Gradient-Schätzung Auswirkungen auf das Update des Parameters hat.</span><span class="sxs-lookup"><span data-stu-id="df4ad-115">It defines how much current stochastic gradient estimate impacts the parameter update.</span></span> <span data-ttu-id="df4ad-116">Die Größe der Delta-Aktualisierungsrate ist proportional zur Lernrate.</span><span class="sxs-lookup"><span data-stu-id="df4ad-116">The size of parameter update delta is proportional to the learning rate.</span></span> <span data-ttu-id="df4ad-117">Kleinere Lernraten Werte führen zu einer langsameren Parameter Entwicklung und einer langsameren Konvergenz, aber übermäßig große Werte von LR können die Konvergenz nicht beeinträchtigt werden, da der gradientenabstieg niemals ein bestimmtes lokales minimal Commit durchführt.</span><span class="sxs-lookup"><span data-stu-id="df4ad-117">Smaller learning rate values lead to slower parameter evolution and slower convergence, but excessively large values of LR may break the convergence altogether as the gradient descent never commits to a particular local minimum.</span></span> <span data-ttu-id="df4ad-118">Obwohl die Lernrate in gewisser Weise vom Trainings Algorithmus angepasst wird, ist es wichtig, einen guten Anfangswert auszuwählen.</span><span class="sxs-lookup"><span data-stu-id="df4ad-118">While learning rate is adaptively adjusted by the training algorithm to some extent, selecting a good initial value for it is important.</span></span> <span data-ttu-id="df4ad-119">Ein üblicher Standard Anfangswert für die Lernrate ist 0,1.</span><span class="sxs-lookup"><span data-stu-id="df4ad-119">A usual default initial value for learning rate is 0.1.</span></span> <span data-ttu-id="df4ad-120">Die Auswahl des besten Werts der Lernrate ist eine gute Grafik (z. b. Abschnitt 4,3 von Goodfellow et al., Deep Learning, mit Press, 2017).</span><span class="sxs-lookup"><span data-stu-id="df4ad-120">Selecting the best value of learning rate is a fine art (see, for example, section 4.3 of Goodfellow et al.,"Deep learning", MIT Press, 2017).</span></span>

### <a name="minibatch-size"></a><span data-ttu-id="df4ad-121">Minibatch-Größe</span><span class="sxs-lookup"><span data-stu-id="df4ad-121">Minibatch size</span></span>

<span data-ttu-id="df4ad-122">Definiert, wie viele Daten Beispiele für eine einzelne Schätzung des stochastischen Farbverlaufs verwendet werden.</span><span class="sxs-lookup"><span data-stu-id="df4ad-122">Defines how many data samples is used for a single estimation of stochastic gradient.</span></span> <span data-ttu-id="df4ad-123">Größere Werte der Mini Batch-Größe führen in der Regel zu stabilerer und mehr monotonischer Konvergenz, können den Trainingsprozess jedoch möglicherweise verlangsamen, da die Kosten einer einzelnen gradientenschätzung proportional zur Minimalgröße sind.</span><span class="sxs-lookup"><span data-stu-id="df4ad-123">Larger values of minibatch size generally lead to more robust and more monotonic convergence but can potentially slow down the training process, as the cost of any one gradient estimation is proportional to the minimatch size.</span></span> <span data-ttu-id="df4ad-124">Ein üblicher Standardwert für die Mini Batch-Größe ist 10.</span><span class="sxs-lookup"><span data-stu-id="df4ad-124">A usual default value for the minibatch size is 10.</span></span>

### <a name="training-epochs-tolerance-gridlocks"></a><span data-ttu-id="df4ad-125">Schulungs Epochen, Toleranz, Gridlocks</span><span class="sxs-lookup"><span data-stu-id="df4ad-125">Training epochs, tolerance, gridlocks</span></span>

<span data-ttu-id="df4ad-126">"Epoche" bedeutet eine komplette Durchführung der geplanten Trainingsdaten.</span><span class="sxs-lookup"><span data-stu-id="df4ad-126">"Epoch" means one complete pass through the scheduled training data.</span></span>
<span data-ttu-id="df4ad-127">Die maximale Anzahl von Epochen pro Trainings Thread (siehe unten) sollte begrenzt sein.</span><span class="sxs-lookup"><span data-stu-id="df4ad-127">The maximum number of epochs per a training thread (see below) should be capped.</span></span> <span data-ttu-id="df4ad-128">Der Trainings Thread wird so definiert, dass er (mit den besten bekannten Kandidaten Parametern) beendet wird, wenn die maximale Anzahl von Epochen ausgeführt wurde.</span><span class="sxs-lookup"><span data-stu-id="df4ad-128">The training thread is defined to terminate (with the best known candidate parameters) when the maximum number of epochs has been run.</span></span> <span data-ttu-id="df4ad-129">Diese Schulung würde jedoch früher beendet werden, wenn die fehl Klassifizierungs Rate bei einem Überprüfungs Zeitplan unter eine ausgewählte Toleranz fällt.</span><span class="sxs-lookup"><span data-stu-id="df4ad-129">However such training would terminate earlier when misclassification rate on validation schedule falls below a chosen tolerance.</span></span> <span data-ttu-id="df4ad-130">Angenommen, die falsche Klassifizierungs Toleranz ist 0,01 (1%); Wenn bei einem Validierungs Satz von 2000 Beispielen weniger als 20 fehl Klassifizierungen angezeigt werden, ist die Toleranzstufe erreicht.</span><span class="sxs-lookup"><span data-stu-id="df4ad-130">Suppose, for example, that misclassification tolerance is 0.01 (1%); if on validation set of 2000 samples we are seeing fewer than 20 misclassifications, then the tolerance level has been achieved.</span></span> <span data-ttu-id="df4ad-131">Ein Trainings Thread wird auch vorzeitig beendet, wenn das Validierungs Ergebnis des Kandidaten Modells in mehreren aufeinander folgenden Epochen (einem gridlock) keine Verbesserung zeigte.</span><span class="sxs-lookup"><span data-stu-id="df4ad-131">A training thread also terminates prematurely if the validation score of the candidate model has not shown any improvement over several consecutive epochs (a gridlock).</span></span> <span data-ttu-id="df4ad-132">Die Logik für das Beenden des gridlock ist zurzeit hart codiert.</span><span class="sxs-lookup"><span data-stu-id="df4ad-132">The logic for the gridlock termination is currently hardcoded.</span></span>

### <a name="measurements-count"></a><span data-ttu-id="df4ad-133">Anzahl von Messungen</span><span class="sxs-lookup"><span data-stu-id="df4ad-133">Measurements count</span></span>

<span data-ttu-id="df4ad-134">Das schätzen der Trainings-/Validierungsergebnisse und der Komponenten des stochastischen-Farbverlaufs auf einem Quantum-Gerät bedeutet, dass Quantum-Status Überschneidungen geschätzt werden, die mehrere Messungen der entsprechenden Observables erfordern.</span><span class="sxs-lookup"><span data-stu-id="df4ad-134">Estimating the training/validation scores and the components of the stochastic gradient on a quantum device amounts to estimating quantum state overlaps that requires multiple measurements of the appropriate observables.</span></span> <span data-ttu-id="df4ad-135">Die Anzahl der Messungen sollte als $O (1/\ Epsilon ^ 2) $ skaliert werden, wobei $ \epsilon $ der gewünschte Schätz Fehler ist.</span><span class="sxs-lookup"><span data-stu-id="df4ad-135">The number of measurements should scale as $O(1/\epsilon^2)$ where $\epsilon$ is the desired estimation error.</span></span>
<span data-ttu-id="df4ad-136">Als Faustregel gilt, dass die anfängliche Anzahl von Messungen ungefähr $1/\ mbox {Tolerance} ^ 2 $ betragen kann (siehe Definition der Toleranz im vorherigen Absatz).</span><span class="sxs-lookup"><span data-stu-id="df4ad-136">As a rule of thumb, the initial measurements count could be approximately $1/\mbox{tolerance}^2$ (see definition of tolerance in the previous paragraph).</span></span> <span data-ttu-id="df4ad-137">Sie müssen die Anzahl der Messungen aufwärts überarbeiten, wenn der Verlaufs Abstieg zu unregelmäßig und die Konvergenz zu schwer zu erreichen ist.</span><span class="sxs-lookup"><span data-stu-id="df4ad-137">One would need to revise the measurement count upward if the gradient descent appears to be too erratic and convergence too hard to achieve.</span></span>

### <a name="training-threads"></a><span data-ttu-id="df4ad-138">Trainieren von Threads</span><span class="sxs-lookup"><span data-stu-id="df4ad-138">Training threads</span></span>

<span data-ttu-id="df4ad-139">Die Wahrscheinlichkeitsfunktion, bei der es sich um das Schulungs Dienstprogramm für den Klassifizierer handelt, ist sehr selten eine Konstante, was bedeutet, dass Sie in der Regel über eine Vielzahl von lokalem Optima im Parameter Bereich verfügt, die sich durch Qualität</span><span class="sxs-lookup"><span data-stu-id="df4ad-139">The likelihood function which is the training utility for the classifier is very seldom convex, meaning that it usually has a multitude of local optima in the parameter space that may differ significantly by quality.</span></span> <span data-ttu-id="df4ad-140">Da der SGD-Prozess nur mit einer bestimmten optimalen konvergiert werden kann, ist es wichtig, mehrere Startparameter Vektoren zu untersuchen.</span><span class="sxs-lookup"><span data-stu-id="df4ad-140">Since the SGD process can converge to only one specific optimum, it is important to explore multiple starting parameter vectors.</span></span> <span data-ttu-id="df4ad-141">Gängige Vorgehensweise beim maschinellen Lernen besteht darin, solche Start Vektoren nach dem Zufallsprinzip zu initialisieren.</span><span class="sxs-lookup"><span data-stu-id="df4ad-141">Common practice in machine learning is to initialize such starting vectors randomly.</span></span> <span data-ttu-id="df4ad-142">Die Q# Trainings-API akzeptiert ein beliebiges Array solcher Start Vektoren, aber der zugrunde liegende Code untersucht sie sequenziell.</span><span class="sxs-lookup"><span data-stu-id="df4ad-142">The Q# training API accepts an arbitrary array of such starting vectors but the underlying code explores them sequentially.</span></span> <span data-ttu-id="df4ad-143">Auf einem Computer mit mehreren Kernen oder in einer beliebigen Parallel Computing-Architektur empfiehlt es sich, mehrere Aufrufe Q# der Trainings-API parallel mit verschiedenen Parameter Initialisierungen über die Aufrufe auszuführen.</span><span class="sxs-lookup"><span data-stu-id="df4ad-143">On a multicore computer or in fact on any parallel computing architecture it is advisable to perform several calls to Q# training API in parallel with different parameter initializations across the calls.</span></span>

#### <a name="how-to-modify-the-hyperparameters"></a><span data-ttu-id="df4ad-144">Ändern der Hyperparameter</span><span class="sxs-lookup"><span data-stu-id="df4ad-144">How to modify the hyperparameters</span></span>

<span data-ttu-id="df4ad-145">In der QML-Bibliothek besteht die beste Möglichkeit zum Ändern der Hyperparameter darin, die Standardwerte des UDT zu überschreiben [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions) .</span><span class="sxs-lookup"><span data-stu-id="df4ad-145">In the QML library, the best way to modify the hyperparameters is by overriding the default values of the UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions).</span></span> <span data-ttu-id="df4ad-146">Zu diesem Zweck wird diese mit der-Funktion aufgerufen [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) und der-Operator angewendet `w/` , um die Standardwerte zu überschreiben.</span><span class="sxs-lookup"><span data-stu-id="df4ad-146">To do this we call it with the function [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) and apply the operator `w/` to override the default values.</span></span> <span data-ttu-id="df4ad-147">Um z. b. 100.000 Messungen und eine Lernrate von 0,01 zu verwenden, verwenden Sie Folgendes:</span><span class="sxs-lookup"><span data-stu-id="df4ad-147">For example, to use 100,000 measurements and a learning rate of 0.01:</span></span>
 ```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
 ```
